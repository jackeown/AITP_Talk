<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css" integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous"/>


		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<style>
			.reveal i{
				font-family: 'FontAwesome';
				font-style: normal;
			}
			.red{
				color:rgb(255, 150, 150);
			}
			.green{
				color:rgb(150, 255, 150);
			}
			.yellow{
				color:rgb(255, 255, 123);
			}
			.row{
				display:flex;
				flex-direction:row;
				justify-content: space-around;
			}
			.box{
				border:1px solid rgb(150, 255, 150);
				padding:0px;
				margin:5px;
				font-size:24pt;
			}
			.emph{
				font-style:italic;
			}
			.ul{
				text-decoration: underline;
			}
			.reveal section img{
				background-color: whitesmoke;
			}

			.table{
				border: 1px solid whitesmoke;
			}

			.table tr:first-child {
				background: rgba(255,255,255,0.5);
				color: black;
			}

			.small {
				font-size: 80%;
			}

			*::selection{
				color: black;
			}

		</style>
		<div class="reveal">
			<div class="slides">
				<section>
					<h3>Reinforcement learning in E</h3>
					<hr>
					<h6 style="text-transform: none">Jack McKeown <span class="green small">(mckeown@cs.miami.edu)</span><br> <span class="small">(advised by Geoff Sutcliffe <span class="green small">(geoff@cs.miami.edu)</span>)</span></h6>
					<aside class="notes">
						Hi everyone.
						My name is Jack McKeown and my advisor is Geoff Sutcliffe.
						Today I'll be presenting our work on Reinforcement Learning in the E theorem prover.
					</aside>
				</section>

				<section>
					<h3>Outline</h3>
					<div class="box" style="padding: 0.5em 0em; width:90%; margin:auto;">
						<ol>
							<li>Previous work in Reinforcement Learning (RL) for ATP</li>
							<li>Incorporation of RL into Eprover</li>
							<li>Results</li>
							<li>Future directions</li>
						</ol>
					</div>
					<aside class="notes">
						First I'm going to talk about some relevant previous and current work others have done.
						Then I'll talk about our approach and our results.
						And I'll finish by discussing future directions for this research.
					</aside>
				</section>

				<section>
					<h3>Previous work in RL for ATP</h3>
					<section>
						<div class="box">
							<img src="images/classification.svg">
						</div>
						<aside class="notes">
							Here is a diagram of the most relevant previous work that I'm aware of.
							The two papers on the left both used the tableaux calculus, whereas the
							others all use some saturation based prover due to their generally superior performance.
						</aside>
					</section>

				</section>

				<section>
					<h3>Incorporation of RL into Eprover</h3>

					<section style="font-size:0.8em">
						<div class="box" style="padding: 1em 0em;">
							<h3>RL in relation to E's Automatic Mode</h3>

							<ul>
									<li class="fragment" data-fragment-index="1"><span class="ul">E's "--auto" mode</span>: <em class="green">features of a problem</em> $\rightarrow$ <em class="green">heuristic</em></li>
									<li class="fragment" data-fragment-index="2"><span class="ul">Our RL idea</span>: <em class="green">state of E</em> $\rightarrow$ <em class="green">probabilistic heuristic</em></li>
									<li class="fragment" data-fragment-index="4"><span class="ul">Usual RL idea</span>: <em class="green">state of E</em> $\rightarrow$ <em class="green">given clause</em></li>
									<br>
							</ul>
	
							<div class="row">
								<textarea cols="50" rows="7" style="overflow: hidden;">
Example heuristic in E's --auto mode:

10 * Clauseweight(PreferGoals,1,1,1),
10 * Clauseweight(PreferNonGoals,1,1,1),
10 * Clauseweight(ByHornDist,1,1,1),
1 * FIFOWeight(ConstPrio)
								</textarea>
	
								<textarea class="fragment" data-fragment-index="3" cols="50" rows="7" style="overflow: hidden;">
Example heuristic in RL mode:

0.5 * Clauseweight(PreferGoals,1,1,1),
0.2 * Clauseweight(PreferNonGoals,1,1,1),
0.1 * Clauseweight(ByHornDist,1,1,1),
0.1 * FIFOWeight(ConstPrio),
...
								</textarea>
	
							</div>
						</div>

						<aside class="notes">
							E's given clause selection is normally guided by a schedule of clause evaluation functions known to E as a "heuristic".<br>
							So a "heuristic" is a schedule of clause evaluation functions.<br>
							A clause evaluation function is an instance of a generic weight function applied to a priority function and other arguments.<br>
							The role of the priority functions is mostly to restrict the clause evaluation function to a certain class of clauses.
							<br><br>
							In E's "--auto" mode, it analyzes the input problem in order to assign a heuristic at the beginning of a proof attempt.
							<br>
							Our idea for Reinforcement Learning was to basically allow this heuristic to be changed over the course of a proof attempt.
							This is why our "actions" are a distribution over clause evaluation functions.<br>
							It's like a randomized version of E's schedule.<br>
							<br>
							This is unlike previous work where typically the action is actually selecting the given clause directly.<br><br>

							One might also ask, "Why use reinforcement learning at all? Can't you just directly train a classifier to pick out useful clauses?"<br>
							One reason is that a selected clause has a complicated effect on the proof search (more than just whether or not it ends up in the proof).
						</aside>

					</section>

					<section>
						<div style="outline:1px solid rgb(150, 255, 150); transform: scale(0.7); padding:1em;top:0px;">
							<h4>Other choices to make</h4>
							<ul>
								<li class="fragment">What are the states, actions, and rewards?</li>
								<li class="fragment">Monte Carlo Tree Search?</li>
								<li class="fragment">Which RL algorithm to use?</li>
								<li class="fragment">Reward shaping?
									<ul>
										<li>
											Optimal policy is preserved when <span class="green">$r := r + \lambda f(s') - f(s)$</span>
										</li>
									</ul>
								<li class="fragment">Problem sampling
									<ul>
										<li>Oversample hard problems? (More meaningful reward signal)</li>
										<li>Oversample easy problems? (Faster training)</li>
									</ul>
								</li>
							</ul>
						</div>
						<aside class="notes">
							Aside from the choice of calculus and prover, here are some other important choices to make.<br><br>

							What should the states, actions, and rewards be?<br>
							Should we use monte carlo tree search to help out the learning as AlphaZero did for solving the game of Go.<br>
							Which RL algorithm should we use? Deep Q Learning? Policy Gradients? Actor Critic? Something Else?<br>
							Related to choosing the rewards themselves is the question of reward shaping which asks
							"How can we change the rewards to make learning faster or smoother, while keeping the same optimal policy"<br>
							Also of concern is what data to use for learning.
							Should we oversample hard problems so that the learned model has been trained to solve hard Problems?
							Or does training on hard problems just cause the learning to be unbearably slow because we have to wait for an entire failed proof attempt.

						</aside>
					</section>

					<section style="padding: 0px 0px;">
						<div style="outline:1px solid rgb(150, 255, 150); transform: scale(0.7); padding:1em; line-height: 1.2em;">
							<h4>The choices we made</h4>
							<ul>
								<li class="fragment">What are the states, actions, and rewards?</li>
								<li class="fragment"><span class="green">state:</span> <span class="red">length and average weight of processed and unprocessed sets</span></li>
								<li class="fragment"><span class="green">actions:</span> <span class="red">choice from a static set of 75 clause evaluation functions (extracted from those used in --auto)</span></li>
								<li class="fragment"><span class="green">rewards:</span> <span class="red">1 if proof found else 0</span></li>
	
								<li class="fragment">Monte Carlo Tree Search? - <span class="green">not yet</span></li>
								<li class="fragment">Which RL algorithm to use? - <span class="green">vanilla policy gradients</span></li>
								<li class="fragment">Reward shaping? - <span class="green">none</span></li>
								<li class="fragment">Problem sampling - <span class="green">uniform</span></li>
							</ul>
						</div>

						<aside class="notes">
							To represent E's state, we used a vector of 4 features: the length and average weight of the unprocessed and processed sets of clauses.<br>
							<br>
							One thing that makes this work different different from other approaches is our choice of action.<br>
							Most other work directly tries to choose the given clause, but our actions are a choice from a static set of 75 clause evaluation functions (extracted from those used in --auto).<br>
							<br>
							For rewards, we simply give a zero for each iteration of the main given clause selection loop unless we have selected the empty clause.
						</aside>
					</section>



					<section>
						<div class="box" style="margin-top: 1em;">
							<img src="images/architecture2.svg" style="width:80%;">
						</div>

						<aside class="notes">
							This is a diagram to help me explain how we structured the learning in our experiment.<br>
							On the left here is our trusty theorem prover E.
							The green boxes are python scripts that guide the learning.<br><br>

							E is invoked with a problem to be solved.<br>
							E itself has been modified to coordinate with gain_xp.py via named pipes.
							E sends its "state" to gain_xp.py,<br>
							gain_xp.py sends an "action" back to E,<br>
							and E sends back a "reward" to gain_xp.py<br><br>

							train_policy.py is used to update the policy according to which gain_xp.py acts.
							It does this using data from previous proof attempts recorded by gain_xp.py
						</aside>
					</section>
				</section>


				<section>
					<h3>Dataset</h3>
					<div class="box" style="padding: 1em 0em;">
						<ul>
							<li>MPTPTP2078 - extracted from Mizar</li>
							<li>Bushy variants of problems</li>
							<li>60 soft seconds given for timeout</li>
						</ul>
					</div>
					<aside class="notes">
						For evaluation of this approach, we used the bushy variants of problems from the MPTPTP2078 dataset which originally come from the Mizar Mathematical Library.
					</aside>
				</section>

				<section>
					<h3>Results</h3>
					<section>
						<div class="box" style="padding: 1em 0em; width:80%; margin:auto;">
							<table class="table">
								<tr><th>RL Policy</th> <th>Problems</th> <th>Percentage</th></tr>
								<tr class="fragment"><td>Uniform Distribution</td> <td>1105</td> <td>53.2%</td></tr>
								<tr class="fragment"><td>Learned Distribution</td> <td>1105</td> <td>53.2%</td></tr>
								<tr class="fragment"><td>Neural Network Policy</td> <td>1110</td> <td>53.4%</td></tr>
								<tr class="fragment" style="background-color: rgba(200,200,255,0.5)"><td>E --auto</td> <td>1156</td> <td>55.6%</td></tr>
							</table>
						</div>
						<aside class="notes">
							As a baseline, we evaluated a policy which just uniformly samples clause evaluation functions for each selection.<br>
							We then compared this to a learned categorical distribution over clause evaluation functions.<br>
							Somewhat surprisingly, this didn't have an appreciable effect on the number of solved problems despite drastically changing the distribution.<br>
							Notice that neither of these policies take the state of E into account, but rather just sample from a constant distribution.<br><br>
							
							Naturally, we'd like to take the state of E into account, so we made a simple neural network in PyTorch to learn a mapping from E's state to a distribution over the clause evaluation functions.<br>
							This distribution is then sampled from to yield the final action taken by this policy.<br>
							This solved another 5 problems, but was still quite underwhelming.<br>
							Finally we can compare all of these to the gold standard of using E's --auto mode which blew us away.
						</aside>
					</section>
				</section>

				<section>
					<h3>Future Directions</h3>
					<div class="box" style="padding: 1em 0em; width:80%; margin:auto;">
						<ul>
							<li>More robust state space</li>
							<li class="fragment">Add Monte Carlo Tree Search (MCTS)</li>
							<li class="fragment">Inspirations from TRAIL</li>
							<li class="fragment">Incorporate problem difficulty into rewards</li>
							<li class="fragment">Retaining the awesomeness of E's <span class="emph green">--auto</span> mode</li>
						</ul>
					</div>

					<aside class="notes">
						The size and average weight of the unprocessed and processed sets is a very minimal representation of E's internal state.<br>
						We chose such a minimal state, because it allows us to bypass embedding clauses, but it's probably too simple to learn an effective reinforcement learning policy.<br><br>
						Monte Carlo Tree Search is used by most previous work and seems like a key ingredient to their success.<br><br>
						The TRAIL paper showed a very interesting, but straightforward way to model the states, actions, and policy that seems very promising.<br><br>
						Perhaps problem difficulty could be incorporated into the rewards so that difficult problems give more reward for solving and less punishment for failing to solve.<br><br>
						Lastly, perhaps RL models could be tuned so that they default to --auto mode, but deviate when advantageous.<br>
					</aside>
				</section>


				<section>
					<h3>Thanks for listening</h3>
					<em class="green">Questions?</em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<em class="green">Suggestions / Advice?</em><br>
				</section>

			</div>
		</div>

		<script src="js/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				plugins: [RevealNotes],
				hash: true,
				math: {
					mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full', // See http://docs.mathjax.org/en/latest/config-files.html
					// pass other options into `MathJax.Hub.Config()`
					TeX: { Macros: { RR: "{\\bf R}" } }
				},

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/highlight/highlight.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true },
				]
			});

		</script>
	</body>
</html>
